# Task 3.2: ìŒì„± ì¸ì‹ (STT - Web Speech API)

## ğŸ“‹ í˜„ì¬ ìƒíƒœ (2026-01-31)

### âœ… ì™„ë£Œëœ í•­ëª©
- âœ… Phase 1-2: í”„ë¡œì íŠ¸ + UI êµ¬ì¡° ì™„ì„±
- âœ… useChat Hook: ë°±ì—”ë“œ ì—°ë™
- âœ… ChatInput ì»´í¬ë„ŒíŠ¸: í…ìŠ¤íŠ¸ ì…ë ¥

### ğŸ”„ ì§„í–‰ ì¤‘ì¸ í•­ëª©
- ğŸ”„ **Phase 3: ìŒì„± ê¸°ëŠ¥** (ì´ íŒŒì¼)
  - Web Speech API (STT)
  - ElevenLabs TTS
  - ì˜¤ë””ì˜¤ ë¹„ì£¼ì–¼ë¼ì´ì €

---

## ğŸ¯ ëª©í‘œ

**ìŒì„± ì¸ì‹ êµ¬í˜„**: Web Speech APIë¡œ í•œêµ­ì–´ ìŒì„± ì…ë ¥ + ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ í‘œì‹œ

---

## ğŸ“ êµ¬í˜„ ê°€ì´ë“œ

### 1. useVoiceRecognition Hook

```typescript
// hooks/useVoiceRecognition.ts

import { useEffect, useRef, useState } from 'react';

interface UseVoiceRecognitionReturn {
  isListening: boolean;
  transcript: string;
  interimTranscript: string;
  startListening: () => void;
  stopListening: () => void;
  resetTranscript: () => void;
  error: string | null;
  isSupported: boolean;
}

export function useVoiceRecognition(): UseVoiceRecognitionReturn {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [interimTranscript, setInterimTranscript] = useState('');
  const [error, setError] = useState<string | null>(null);

  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);

  const isSupported =
    typeof window !== 'undefined' &&
    ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window);

  useEffect(() => {
    if (!isSupported) return;

    const SpeechRecognition =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;
    const recognition = new SpeechRecognition();

    // ì„¤ì •
    recognition.lang = 'ko-KR';
    recognition.continuous = true;
    recognition.interimResults = true;

    // ê²°ê³¼ ì²˜ë¦¬
    recognition.onresult = (event) => {
      let interim = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;

        if (event.results[i].isFinal) {
          setTranscript((prev) => prev + transcript + ' ');
        } else {
          interim += transcript;
        }
      }

      setInterimTranscript(interim);

      // ìë™ ì¢…ë£Œ: 3ì´ˆ ë¬´ìŒ ê°ì§€
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
      }

      silenceTimerRef.current = setTimeout(() => {
        recognition.stop();
      }, 3000);
    };

    // ì—ëŸ¬ ì²˜ë¦¬
    recognition.onerror = (event) => {
      const errorMap: Record<string, string> = {
        'not-allowed': 'ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”',
        'no-speech': 'ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',
        network: 'ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”',
        aborted: 'ìŒì„± ì¸ì‹ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤',
      };

      const message =
        errorMap[event.error] || `ì—ëŸ¬: ${event.error}`;
      setError(message);
      setIsListening(false);
    };

    recognition.onend = () => {
      setIsListening(false);
    };

    recognitionRef.current = recognition;

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.abort();
      }
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
      }
    };
  }, [isSupported]);

  const startListening = () => {
    if (!recognitionRef.current || isListening) return;

    setError(null);
    setTranscript('');
    setInterimTranscript('');
    setIsListening(true);

    recognitionRef.current.start();
  };

  const stopListening = () => {
    if (!recognitionRef.current) return;

    recognitionRef.current.stop();
    setIsListening(false);

    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
    }
  };

  const resetTranscript = () => {
    setTranscript('');
    setInterimTranscript('');
  };

  return {
    isListening,
    transcript,
    interimTranscript,
    startListening,
    stopListening,
    resetTranscript,
    error,
    isSupported,
  };
}
```

### 2. ChatInputê³¼ í†µí•©

```typescript
// components/chat/ChatInput.tsx ìˆ˜ì •

import { useVoiceRecognition } from '@/hooks/useVoiceRecognition';

interface ChatInputProps {
  onSend: (message: string) => void;
  disabled?: boolean;
}

export function ChatInput({
  onSend,
  disabled = false,
}: ChatInputProps) {
  const [input, setInput] = useState('');
  const {
    isListening,
    transcript,
    interimTranscript,
    startListening,
    stopListening,
    resetTranscript,
  } = useVoiceRecognition();

  const handleMicStart = () => {
    setInput(''); // ì´ì „ ì…ë ¥ ì´ˆê¸°í™”
    resetTranscript();
    startListening();
  };

  const handleMicEnd = () => {
    stopListening();

    // ìµœì¢… í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ì°½ì— ì¶”ê°€
    const finalText = transcript.trim();
    if (finalText) {
      setInput(finalText);
      onSend(finalText);
      resetTranscript();
    }
  };

  const handleSend = () => {
    if (input.trim()) {
      onSend(input);
      setInput('');
    }
  };

  return (
    <div className="p-4 border-t border-white/10 space-y-2">
      {/* ë…¹ìŒ ì¤‘: ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° */}
      {isListening && (
        <div className="px-4 py-3 rounded-lg
          bg-blue-500/10 border border-blue-500/30">
          <p className="text-sm text-blue-300 mb-1">ìŒì„± ì…ë ¥ ì¤‘...</p>
          {interimTranscript && (
            <p className="text-white">{interimTranscript}</p>
          )}
        </div>
      )}

      {/* ì…ë ¥ ì˜ì—­ */}
      <div className="flex gap-3 items-end">
        {/* í…ìŠ¤íŠ¸ ì…ë ¥ */}
        <textarea
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyDown={(e) => {
            if (e.key === 'Enter' && !e.shiftKey) {
              e.preventDefault();
              handleSend();
            }
          }}
          placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
          disabled={disabled || isListening}
          className="flex-1 px-4 py-3 rounded-lg
            bg-white/5 border border-white/10
            text-white placeholder:text-white/40
            resize-none max-h-24
            disabled:opacity-50"
          rows={1}
        />

        {/* ë§ˆì´í¬ ë²„íŠ¼ */}
        <button
          onMouseDown={handleMicStart}
          onMouseUp={handleMicEnd}
          onTouchStart={handleMicStart}
          onTouchEnd={handleMicEnd}
          disabled={disabled}
          className={`p-3 rounded-lg transition-all
            ${
              isListening
                ? 'bg-red-500 text-white animate-pulse'
                : 'bg-white/10 text-white hover:bg-white/20'
            }
            disabled:opacity-50`}
        >
          <Mic className="w-5 h-5" />
        </button>

        {/* Send ë²„íŠ¼ */}
        <button
          onClick={handleSend}
          disabled={!input.trim() || disabled || isListening}
          className="p-3 rounded-lg bg-cyan-500 text-white
            hover:bg-cyan-600 transition-all
            disabled:opacity-50 disabled:cursor-not-allowed"
        >
          <Send className="w-5 h-5" />
        </button>
      </div>
    </div>
  );
}
```

---

## ğŸ¤ ì—ëŸ¬ ì²˜ë¦¬

| ì—ëŸ¬ | ì‚¬ìš©ì ë©”ì‹œì§€ | í•´ê²°ì±… |
|------|--------------|-------|
| `not-allowed` | ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš” | ì„¤ì •ì—ì„œ ê¶Œí•œ í™•ì¸ |
| `no-speech` | ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ | ë‹¤ì‹œ ì‹œë„ |
| `network` | ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš” | ì—°ê²° í™•ì¸ í›„ ì¬ì‹œë„ |

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] useVoiceRecognition Hook êµ¬í˜„
- [ ] í•œêµ­ì–´(ko-KR) ì–¸ì–´ ì„¤ì • í™•ì¸
- [ ] ìë™ ì¢…ë£Œ (3ì´ˆ ë¬´ìŒ)
- [ ] ì—ëŸ¬ ì²˜ë¦¬ UI
- [ ] ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
- [ ] ë§ˆì´í¬ ë²„íŠ¼ ë¡±í”„ë ˆìŠ¤ ë™ì‘
- [ ] ë¸Œë¼ìš°ì € í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- `task-3.3-voice-synthesis.md` - TTS (ìŒì„± ì¶œë ¥)
- `task-3.4-audio-visualizer.md` - ì˜¤ë””ì˜¤ ì‹œê°í™”
- Web Speech API MDN ë¬¸ì„œ

---

**ìƒíƒœ**: ğŸŸ¡ Phase 3 ì§„í–‰ ì¤‘ (STT)
**ë‹¤ìŒ**: task-3.3 (TTS), task-3.4 (ì˜¤ë””ì˜¤ ë¹„ì£¼ì–¼ë¼ì´ì €)
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-01-31
