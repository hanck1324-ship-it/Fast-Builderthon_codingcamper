# Task 3.3: ìŒì„± í•©ì„± (TTS - ElevenLabs)

## ğŸ“‹ í˜„ì¬ ìƒíƒœ (2026-01-31)

### âœ… ì™„ë£Œëœ í•­ëª©
- âœ… Backend/main.py: FastAPI ê¸°ë³¸ êµ¬ì¡° + TTS ì—”ë“œí¬ì¸íŠ¸ ìŠ¤í…
- âœ… lib/api.ts: synthesizeVoice() í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ
- âœ… task-3.1: Claude API + LangChain ê°€ì´ë“œ

### ğŸ”„ ì§„í–‰ ì¤‘ì¸ í•­ëª©
- ğŸ”„ **Phase 3: TTS ìŒì„± í•©ì„±** (ì´ íŒŒì¼)
  - ElevenLabs API ì—°ë™
  - useTextToSpeech Hook
  - ì¬ìƒ ìˆœì„œ ê´€ë¦¬

---

## ğŸ¯ ëª©í‘œ

**ElevenLabs TTS êµ¬í˜„**: AI ì‘ë‹µì„ ìŒì„±ìœ¼ë¡œ ìë™ ì¬ìƒ

---

## ğŸ”‘ ì£¼ìš” ê¸°ëŠ¥

### 1. ë°±ì—”ë“œ: Voice Service (FastAPI)

```python
# Backend/services/voice_service.py

from elevenlabs import ElevenLabs, VoiceSettings
from typing import Literal
import os

class VoiceService:
    def __init__(self):
        self.client = ElevenLabs(
            api_key=os.getenv("ELEVENLABS_API_KEY")
        )
        self.voices = {
            "james": os.getenv("JAMES_VOICE_ID", "pNInz6obpgDQGcFmaJgB"),
            "linda": os.getenv("LINDA_VOICE_ID", "21m00Tcm4TlvDq8ikWAM")
        }

    async def synthesize(
        self,
        text: str,
        speaker: Literal["james", "linda"]
    ) -> bytes:
        """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜"""
        voice_id = self.voices[speaker]

        audio = self.client.generate(
            text=text,
            voice=voice_id,
            model="eleven_multilingual_v2",
            voice_settings=VoiceSettings(
                stability=0.5,
                similarity_boost=0.75,
                style=0.5,
                use_speaker_boost=True
            )
        )

        # Generatorë¥¼ bytesë¡œ ë³€í™˜
        audio_bytes = b"".join(audio)
        return audio_bytes

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_voice_service = VoiceService()

async def get_voice_service() -> VoiceService:
    return _voice_service
```

### 2. ë°±ì—”ë“œ: API ì—”ë“œí¬ì¸íŠ¸

```python
# Backend/app/api/v1/voice.py

from fastapi import APIRouter, Response, Depends
from pydantic import BaseModel
from typing import Literal
from app.services.voice_service import get_voice_service, VoiceService

router = APIRouter()

class SynthesizeRequest(BaseModel):
    text: str
    speaker: Literal["james", "linda"]

@router.post("/synthesize")
async def synthesize_speech(
    request: SynthesizeRequest,
    voice_service: VoiceService = Depends(get_voice_service)
) -> Response:
    """ìŒì„± í•©ì„±"""
    try:
        audio_bytes = await voice_service.synthesize(
            text=request.text,
            speaker=request.speaker
        )

        return Response(
            content=audio_bytes,
            media_type="audio/mpeg",
            headers={
                "Content-Disposition": f"attachment; filename={request.speaker}.mp3"
            }
        )
    except Exception as e:
        return Response(
            content={"error": str(e)},
            status_code=500,
            media_type="application/json"
        )
```

### 3. í”„ë¡ íŠ¸ì—”ë“œ: useTextToSpeech Hook

```typescript
// hooks/useTextToSpeech.ts

import { useState, useRef } from 'react';
import { api } from '@/lib/api';

interface UseTextToSpeechReturn {
  speak: (text: string, speaker: 'james' | 'linda') => Promise<void>;
  isPlaying: boolean;
  stop: () => void;
  currentSpeaker: 'james' | 'linda' | null;
}

export function useTextToSpeech(): UseTextToSpeechReturn {
  const [isPlaying, setIsPlaying] = useState(false);
  const [currentSpeaker, setCurrentSpeaker] = useState<
    'james' | 'linda' | null
  >(null);

  const audioRef = useRef<HTMLAudioElement | null>(null);

  const speak = async (
    text: string,
    speaker: 'james' | 'linda'
  ): Promise<void> => {
    return new Promise((resolve) => {
      try {
        setIsPlaying(true);
        setCurrentSpeaker(speaker);

        // API í˜¸ì¶œë¡œ ìŒì„± ë°›ê¸°
        api
          .synthesizeVoice(text, speaker)
          .then((audioBuffer) => {
            // Blob ìƒì„±
            const audioBlob = new Blob([audioBuffer], {
              type: 'audio/mpeg',
            });
            const audioUrl = URL.createObjectURL(audioBlob);

            // Audio ì—˜ë¦¬ë¨¼íŠ¸ ìƒì„±/ì¬ì‚¬ìš©
            let audio = audioRef.current;
            if (!audio) {
              audio = new Audio();
              audioRef.current = audio;
            }

            audio.src = audioUrl;

            // ì¬ìƒ ì¢…ë£Œ ì´ë²¤íŠ¸
            const onEnded = () => {
              setIsPlaying(false);
              setCurrentSpeaker(null);
              URL.revokeObjectURL(audioUrl);
              audio!.removeEventListener('ended', onEnded);
              resolve();
            };

            audio.addEventListener('ended', onEnded);

            // ì¬ìƒ ì‹œì‘
            audio.play().catch((err) => {
              console.error('Audio play error:', err);
              setIsPlaying(false);
              setCurrentSpeaker(null);
              resolve();
            });
          })
          .catch((err) => {
            console.error('TTS error:', err);
            setIsPlaying(false);
            setCurrentSpeaker(null);
            resolve();
          });
      } catch (error) {
        console.error('Unexpected error:', error);
        setIsPlaying(false);
        setCurrentSpeaker(null);
        resolve();
      }
    });
  };

  const stop = () => {
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current.currentTime = 0;
      setIsPlaying(false);
      setCurrentSpeaker(null);
    }
  };

  return {
    speak,
    isPlaying,
    stop,
    currentSpeaker,
  };
}
```

### 4. DebateRoom í†µí•©

```typescript
// components/debate/DebateRoom.tsx ìˆ˜ì •

'use client';

import { useChat } from '@/hooks/useChat';
import { useTextToSpeech } from '@/hooks/useTextToSpeech';
import { ChatMessages } from '@/components/chat/ChatMessages';

export function DebateRoom(props: DebateRoomProps) {
  const { messages, handleSendMessage, isLoadingFromBackend } = useChat(props);
  const { speak, isPlaying, currentSpeaker } = useTextToSpeech();

  const handleMessageReceived = async (
    jamesResponse: string,
    lindaResponse: string
  ) => {
    // James ì‘ë‹µ ì¬ìƒ
    await speak(jamesResponse, 'james');

    // Linda ì‘ë‹µ ì¬ìƒ
    await speak(lindaResponse, 'linda');
  };

  return (
    <div className="flex flex-col h-screen">
      {/* ë©”ì‹œì§€ í‘œì‹œ + í˜„ì¬ ì¬ìƒ ì¤‘ì¸ speaker í‘œì‹œ */}
      <ChatMessages
        messages={messages}
        isLoading={isLoadingFromBackend}
        currentSpeaker={isPlaying ? currentSpeaker : null}
      />

      {/* ë‚˜ë¨¸ì§€ UI */}
    </div>
  );
}
```

---

## ğŸµ ìŒì„± ì„¤ì •

| AI | ì¶”ì²œ ìŒì„± | Voice ID | íŠ¹ì§• |
|----|----------|----------|------|
| James | Adam (ë‚¨ì„±) | `pNInz6obpgDQGcFmaJgB` | ì°¨ë¶„í•˜ê³  ë…¼ë¦¬ì  |
| Linda | Rachel (ì—¬ì„±) | `21m00Tcm4TlvDq8ikWAM` | ë°ê³  ë”°ëœ»í•¨ |

---

## ğŸ”§ í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
# Backend/.env
ELEVENLABS_API_KEY=sk_...
JAMES_VOICE_ID=pNInz6obpgDQGcFmaJgB
LINDA_VOICE_ID=21m00Tcm4TlvDq8ikWAM
```

```typescript
// frontend/lib/api.ts
async synthesizeVoice(
  text: string,
  speaker: 'james' | 'linda'
): Promise<ArrayBuffer> {
  const response = await fetch(
    `${this.baseUrl}/api/v1/voice/synthesize`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text, speaker }),
    }
  );

  if (!response.ok) {
    throw new APIError('TTS failed', response.status);
  }

  return response.arrayBuffer();
}
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ElevenLabs API Key í™•ë³´
- [ ] VoiceService í´ë˜ìŠ¤ êµ¬í˜„
- [ ] POST /api/v1/voice/synthesize ì—”ë“œí¬ì¸íŠ¸
- [ ] useTextToSpeech Hook êµ¬í˜„
- [ ] lib/api.tsì— synthesizeVoice() ì¶”ê°€
- [ ] DebateRoomì—ì„œ ìë™ ì¬ìƒ
- [ ] ì˜¤ë¥˜ ì²˜ë¦¬ (ë„¤íŠ¸ì›Œí¬, API ì˜¤ë¥˜)

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- `task-3.2-voice-recognition.md` - STT
- `task-3.4-audio-visualizer.md` - ì˜¤ë””ì˜¤ ì‹œê°í™”
- ElevenLabs API ë¬¸ì„œ

---

**ìƒíƒœ**: ğŸŸ¡ Phase 3 ì§„í–‰ ì¤‘ (TTS)
**ë‹¤ìŒ**: task-3.4 (ì˜¤ë””ì˜¤ ë¹„ì£¼ì–¼ë¼ì´ì €)
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-01-31
