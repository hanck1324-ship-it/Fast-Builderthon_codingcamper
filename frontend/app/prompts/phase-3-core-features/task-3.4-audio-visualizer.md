# Task 3.4: ì˜¤ë””ì˜¤ ë¹„ì£¼ì–¼ë¼ì´ì €

## ğŸ“‹ í˜„ì¬ ìƒíƒœ (2026-01-31)

### âœ… ì™„ë£Œëœ í•­ëª©
- âœ… useVoiceRecognition Hook (STT)
- âœ… useTextToSpeech Hook (TTS)
- âœ… ChatInput ì»´í¬ë„ŒíŠ¸ (ë§ˆì´í¬ ë²„íŠ¼)

### ğŸ”„ ì§„í–‰ ì¤‘ì¸ í•­ëª©
- ğŸ”„ **Phase 3: ì˜¤ë””ì˜¤ ì‹œê°í™”** (ì´ íŒŒì¼)
  - ë…¹ìŒ ì¤‘: ì…ë ¥ íŒŒí˜•
  - ì¬ìƒ ì¤‘: ì¶œë ¥ íŒŒí˜•

---

## ğŸ¯ ëª©í‘œ

**ì˜¤ë””ì˜¤ ë¹„ì£¼ì–¼ë¼ì´ì € êµ¬í˜„**: ì‹¤ì‹œê°„ ì£¼íŒŒìˆ˜ ë¶„ì„ + ë™ì  ë§‰ëŒ€ ì• ë‹ˆë©”ì´ì…˜

---

## ğŸ“ êµ¬í˜„ ê°€ì´ë“œ

### 1. AudioWaveform.tsx

```typescript
// components/ui/AudioWaveform.tsx

import { useEffect, useRef, useState } from 'react';
import { motion } from 'framer-motion';

interface AudioWaveformProps {
  type: 'recording' | 'playback';
  isActive: boolean;
  color?: string;
  barCount?: number;
  audioElement?: HTMLAudioElement | null;
  mediaStream?: MediaStream | null;
}

export function AudioWaveform({
  type,
  isActive,
  color = '#00d4ff',
  barCount = 20,
  audioElement,
  mediaStream,
}: AudioWaveformProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const animationRef = useRef<number | null>(null);
  const [barHeights, setBarHeights] = useState<number[]>(
    Array(barCount).fill(0)
  );

  useEffect(() => {
    if (!isActive) {
      // ë¹„í™œì„± ìƒíƒœ: ë‚®ì€ ë†’ì´
      setBarHeights(Array(barCount).fill(2));
      return;
    }

    try {
      // AudioContext ìƒì„±
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext ||
          (window as any).webkitAudioContext)();
      }

      const audioContext = audioContextRef.current;
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 64;
      analyserRef.current = analyser;

      if (type === 'recording' && mediaStream) {
        // ë…¹ìŒ ëª¨ë“œ: MediaStreamSource ì—°ê²°
        const source = audioContext.createMediaStreamSource(
          mediaStream
        );
        source.connect(analyser);
      } else if (type === 'playback' && audioElement) {
        // ì¬ìƒ ëª¨ë“œ: Audio ì—˜ë¦¬ë¨¼íŠ¸ ì—°ê²°
        const source =
          audioContext.createMediaElementSource(audioElement);
        source.connect(analyser);
        analyser.connect(audioContext.destination);
      }

      // ì• ë‹ˆë©”ì´ì…˜ ë£¨í”„
      const draw = () => {
        const dataArray = new Uint8Array(
          analyser.frequencyBinCount
        );
        analyser.getByteFrequencyData(dataArray);

        // ë§‰ëŒ€ ë†’ì´ ê³„ì‚° (ë¶€ë“œëŸ¬ìš´ ë³´ê°„)
        const newHeights = Array(barCount)
          .fill(0)
          .map((_, i) => {
            const index = Math.floor(
              (i / barCount) * dataArray.length
            );
            const value = dataArray[index];

            // ì •ê·œí™”: 0-24px ë²”ìœ„
            const height = (value / 255) * 24;

            return Math.max(2, Math.min(24, height));
          });

        setBarHeights(newHeights);
        animationRef.current = requestAnimationFrame(draw);
      };

      draw();

      return () => {
        if (animationRef.current) {
          cancelAnimationFrame(animationRef.current);
        }
      };
    } catch (error) {
      console.error('Audio Waveform error:', error);
    }
  }, [isActive, type, mediaStream, audioElement, barCount]);

  return (
    <div className="flex items-center justify-center gap-1 h-12">
      {barHeights.map((height, i) => (
        <motion.div
          key={i}
          className="rounded-sm"
          style={{
            width: 3,
            height: `${height}px`,
            backgroundColor: color,
            minHeight: 4,
            maxHeight: 24,
            opacity: isActive ? 1 : 0.3,
          }}
          animate={{
            height: `${height}px`,
          }}
          transition={{
            type: 'spring',
            stiffness: 300,
            damping: 30,
          }}
        />
      ))}
    </div>
  );
}
```

### 2. ChatInput with Waveform

```typescript
// components/chat/ChatInput.tsx ìˆ˜ì •

import { AudioWaveform } from '@/components/ui/AudioWaveform';
import { useVoiceRecognition } from '@/hooks/useVoiceRecognition';

interface ChatInputProps {
  onSend: (message: string) => void;
  disabled?: boolean;
}

export function ChatInput({
  onSend,
  disabled = false,
}: ChatInputProps) {
  const [input, setInput] = useState('');
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const {
    isListening,
    transcript,
    startListening,
    stopListening,
  } = useVoiceRecognition();

  const handleMicStart = async () => {
    try {
      // MediaStream íšë“ (ì˜¤ë””ì˜¤ ë¹„ì£¼ì–¼ë¼ì´ì €ìš©)
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: true,
      });
      mediaStreamRef.current = stream;

      // ìŒì„± ì¸ì‹ ì‹œì‘
      startListening();
    } catch (error) {
      console.error('Microphone access error:', error);
    }
  };

  const handleMicEnd = () => {
    stopListening();

    // ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach((track) => {
        track.stop();
      });
      mediaStreamRef.current = null;
    }

    // í…ìŠ¤íŠ¸ ì „ì†¡
    if (transcript.trim()) {
      onSend(transcript);
      setInput('');
    }
  };

  return (
    <div className="p-4 border-t border-white/10 space-y-3">
      {/* ë…¹ìŒ ì¤‘: ì˜¤ë””ì˜¤ íŒŒí˜• */}
      {isListening && (
        <AudioWaveform
          type="recording"
          isActive={true}
          color="#00d4ff"
          barCount={20}
          mediaStream={mediaStreamRef.current}
        />
      )}

      {/* ì…ë ¥ ì˜ì—­ */}
      <div className="flex gap-3 items-end">
        <textarea
          value={input}
          onChange={(e) => setInput(e.target.value)}
          placeholder="ë©”ì‹œì§€ ì…ë ¥..."
          disabled={disabled || isListening}
          className="flex-1 px-4 py-3 rounded-lg
            bg-white/5 border border-white/10
            text-white resize-none max-h-24"
        />

        {/* ë§ˆì´í¬ ë²„íŠ¼ */}
        <button
          onMouseDown={handleMicStart}
          onMouseUp={handleMicEnd}
          onTouchStart={handleMicStart}
          onTouchEnd={handleMicEnd}
          className={`p-3 rounded-lg transition-all
            ${
              isListening
                ? 'bg-red-500 text-white animate-pulse'
                : 'bg-white/10 text-white hover:bg-white/20'
            }`}
        >
          <Mic className="w-5 h-5" />
        </button>

        {/* Send ë²„íŠ¼ */}
        <button
          onClick={() => onSend(input)}
          disabled={!input.trim() || disabled}
          className="p-3 rounded-lg bg-cyan-500 text-white
            hover:bg-cyan-600 transition-all
            disabled:opacity-50"
        >
          <Send className="w-5 h-5" />
        </button>
      </div>
    </div>
  );
}
```

### 3. DebateRoom with Playback Waveform

```typescript
// components/debate/DebateRoom.tsx ìˆ˜ì •

import { AudioWaveform } from '@/components/ui/AudioWaveform';
import { useTextToSpeech } from '@/hooks/useTextToSpeech';

export function DebateRoom(props: DebateRoomProps) {
  const { speak, isPlaying, currentSpeaker } = useTextToSpeech();
  const audioRef = useRef<HTMLAudioElement>(null);

  return (
    <div className="flex flex-col h-screen">
      {/* ë©”ì‹œì§€ */}
      <ChatMessages messages={messages} />

      {/* AI ìŒì„± ì¬ìƒ ì¤‘: íŒŒí˜• í‘œì‹œ */}
      {isPlaying && currentSpeaker && (
        <div className="px-4 py-2 bg-white/5 border-t border-white/10">
          <p className="text-sm text-white/60 mb-2">
            {currentSpeaker === 'james' ? 'ğŸ­' : 'ğŸŒŸ'}{' '}
            {currentSpeaker === 'james' ? 'James' : 'Linda'} ìŒì„± ì¤‘...
          </p>
          <AudioWaveform
            type="playback"
            isActive={isPlaying}
            color={
              currentSpeaker === 'james' ? '#ff4757' : '#2ed573'
            }
            audioElement={audioRef.current}
          />
        </div>
      )}

      {/* Input */}
      <ChatInput onSend={handleSend} disabled={isLoadingFromBackend} />

      {/* ìˆ¨ê²¨ì§„ Audio ì—˜ë¦¬ë¨¼íŠ¸ (ì¬ìƒìš©) */}
      <audio ref={audioRef} style={{ display: 'none' }} />
    </div>
  );
}
```

---

## ğŸ¨ ìƒ‰ìƒ ë§¤í•‘

```typescript
const colors = {
  recording: '#00d4ff',    // Cyan (ì‚¬ìš©ì ë…¹ìŒ)
  james: '#ff4757',        // Red (James ìŒì„±)
  linda: '#2ed573',        // Green (Linda ìŒì„±)
};
```

---

## âš™ï¸ AudioContext ìµœì í™”

```typescript
// ì—¬ëŸ¬ Waveformì´ ê°™ì€ AudioContext ì‚¬ìš© ì‹œ
const audioContextRef = useRef<AudioContext | null>(null);

// useEffect ì‹œì‘
if (!audioContextRef.current) {
  audioContextRef.current = new (window.AudioContext ||
    (window as any).webkitAudioContext)();
}
const audioContext = audioContextRef.current;

// ... analyser ì—°ê²°

// cleanup: AudioContext ì¢…ë£Œ X (ë‹¤ë¥¸ ê³³ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)
// ëŒ€ì‹  analyser ì—°ê²°ë§Œ ì œê±°
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] AudioWaveform ì»´í¬ë„ŒíŠ¸ êµ¬í˜„
- [ ] Web Audio API AnalyserNode ì—°ê²°
- [ ] ë…¹ìŒ ì‹œ MediaStream ì£¼íŒŒìˆ˜ ë¶„ì„
- [ ] ì¬ìƒ ì‹œ Audio ì—˜ë¦¬ë¨¼íŠ¸ ì£¼íŒŒìˆ˜ ë¶„ì„
- [ ] ë¶€ë“œëŸ¬ìš´ ì• ë‹ˆë©”ì´ì…˜ (spring ë¬¼ë¦¬)
- [ ] 3ê°€ì§€ ìƒ‰ìƒ êµ¬ë¶„ (recording, james, linda)
- [ ] AudioContext ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- `task-3.2-voice-recognition.md` - STT
- `task-3.3-voice-synthesis.md` - TTS
- Web Audio API MDN ë¬¸ì„œ

---

**ìƒíƒœ**: ğŸŸ¡ Phase 3 ì§„í–‰ ì¤‘ (ì˜¤ë””ì˜¤ ì‹œê°í™”)
**ë‹¤ìŒ**: Phase 4 (ë°ì´í„°ë² ì´ìŠ¤)
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-01-31
