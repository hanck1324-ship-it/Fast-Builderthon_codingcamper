"""
AI í† ë¡  ì—”ì§„ ì„œë¹„ìŠ¤
NVIDIA NIM + LangChainì„ ì‚¬ìš©í•œ 3ì í† ë¡  AI ì—”ì§„
"""
from typing import Optional, Dict, List, Tuple
from pathlib import Path
import logging
import json

from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

from app.core.config import settings
from app.models.schemas import DebaterRole


logger = logging.getLogger(__name__)


class TokenCalculator:
    """í† í° ê³„ì‚° ë¡œì§"""
    
    BASE_TOKENS = 10
    LONG_MESSAGE_TOKENS = 20  # 50ì ì´ìƒ
    QUESTION_TOKENS = 15
    
    @classmethod
    def calculate(cls, message: str) -> int:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ë”°ë¥¸ í† í° ê³„ì‚°
        
        - ê¸°ë³¸ ë°œì–¸: +10 í† í°
        - 50ì ì´ìƒ ë…¼ë¦¬ì  ë°œì–¸: +20 í† í°
        - ì§ˆë¬¸ í˜•íƒœ ë°œì–¸: +15 í† í°
        """
        tokens = cls.BASE_TOKENS
        
        # 50ì ì´ìƒ ë©”ì‹œì§€
        if len(message) >= 50:
            tokens = cls.LONG_MESSAGE_TOKENS
        
        # ì§ˆë¬¸ í˜•íƒœ (?, ê¹Œìš”, ëŠ”ì§€, ì¼ê¹Œ ë“±)
        question_markers = ['?', 'ê¹Œìš”', 'ë‚˜ìš”', 'ëŠ”ì§€', 'ì¼ê¹Œ', 'í• ê¹Œ', 'ì„ê¹Œ']
        if any(marker in message for marker in question_markers):
            tokens = max(tokens, cls.QUESTION_TOKENS)
        
        return tokens


class DebateEngine:
    """
    AI í† ë¡  ì—”ì§„
    NVIDIA NIM + LangChainìœ¼ë¡œ êµ¬í˜„ëœ 3ì í† ë¡  ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        self.sessions: Dict[str, dict] = {}
        self.james_prompt: Optional[str] = None
        self.linda_prompt: Optional[str] = None
        
        # LangChain ë©”ëª¨ë¦¬ (ì„¸ì…˜ë³„ë¡œ ê´€ë¦¬)
        self.james_memories: Dict[str, ConversationBufferWindowMemory] = {}
        self.linda_memories: Dict[str, ConversationBufferWindowMemory] = {}
        
        # NVIDIA LLM ì´ˆê¸°í™”
        self.llm: Optional[ChatNVIDIA] = None
        self._init_llm()
        self._load_prompts()
    
    def _init_llm(self):
        """NVIDIA ChatNVIDIA LLM ì´ˆê¸°í™”"""
        if settings.NVIDIA_API_KEY:
            try:
                self.llm = ChatNVIDIA(
                    model="ai-llama-3_3-70b-instruct",
                    nvidia_api_key=settings.NVIDIA_API_KEY,
                    temperature=0.7,
                    max_tokens=256,
                )
                logger.info("NVIDIA LLM ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"NVIDIA LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.llm = None
        else:
            logger.warning("NVIDIA_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def _load_prompts(self):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
        prompts_dir = Path(__file__).parent.parent / "prompts"
        
        james_path = prompts_dir / "james.txt"
        linda_path = prompts_dir / "linda.txt"
        
        if james_path.exists():
            self.james_prompt = james_path.read_text(encoding="utf-8")
        else:
            self.james_prompt = self._get_default_james_prompt()
        
        if linda_path.exists():
            self.linda_prompt = linda_path.read_text(encoding="utf-8")
        else:
            self.linda_prompt = self._get_default_linda_prompt()
    
    def _get_default_james_prompt(self) -> str:
        """ì œì„ìŠ¤ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸"""
        return """ë‹¹ì‹ ì€ 'ì œì„ìŠ¤', ë¹„íŒì  ì‚¬ê³ ë¥¼ ì¤‘ì‹œí•˜ëŠ” í† ë¡  AIì…ë‹ˆë‹¤.

## ì„±ê²©
- ë…¼ë¦¬ì ì´ê³  ë¶„ì„ì 
- ì§ì„¤ì ì´ì§€ë§Œ ì¡´ì¤‘í•˜ëŠ” í†¤
- ê±´ì„¤ì ì¸ ë¹„íŒ ì œê³µ

## ì—­í• 
1. ì‚¬ìš©ì ì£¼ì¥ì˜ ì•½ì ì´ë‚˜ ë¹ˆí‹ˆì„ ì°¾ì•„ ì§ˆë¬¸
2. ë°˜ëŒ€ ê´€ì ì´ë‚˜ ë°˜ë¡€ ì œì‹œ
3. ë…¼ë¦¬ì  ê°œì„ ì  ì œì•ˆ
4. ì†Œí¬ë¼í…ŒìŠ¤ì‹ ë¬¸ë‹µë²• ì ìš©: ì™œ/ì–´ë–»ê²Œ/ê·¼ê±°ëŠ”? ì§ˆë¬¸ìœ¼ë¡œ ì‚¬ìš©ìê°€ ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ê²Œ ìœ ë„

## í† ë¡  ì´ˆì 
- í† ë¡  ì£¼ì œ: {topic}
- ì‚¬ìš©ì ì…ì¥: {user_position}
- ë‹¹ì‹ ì˜ ì…ì¥: {ai_position}
- ë‹µë³€ì€ ë°˜ë“œì‹œ í† ë¡  ì£¼ì œì™€ ì‚¬ìš©ì ë°œì–¸ì— ì§ì ‘ ì—°ê²°
- ì£¼ì œì™€ ë¬´ê´€í•œ ë‚´ìš© ê¸ˆì§€, ì‚¬ìš©ìê°€ ë²—ì–´ë‚˜ë©´ 1ë¬¸ì¥ìœ¼ë¡œ ê´€ë ¨ì„ ë¬»ê³  1ë¬¸ì¥ìœ¼ë¡œ ì£¼ì œì— ë§ëŠ” ë…¼ì ì„ ì œì‹œ

## ì œì•½
- ë°˜ë“œì‹œ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€
- í•œêµ­ì–´ë¡œ ëŒ€í™”
- ì¸ì‹ ê³µê²© ê¸ˆì§€, ì•„ì´ë””ì–´ì—ë§Œ ì§‘ì¤‘
- ë„ˆë¬´ ë¶€ì •ì ì´ì§€ ì•Šê²Œ, ë°œì „ì  ë°©í–¥ ì œì‹œ
- ë©”íƒ€ ë°œì–¸(ì‹œìŠ¤í…œ/í”„ë¡¬í”„íŠ¸/ì—­í•  ì–¸ê¸‰) ê¸ˆì§€
- 1~2ê°œì˜ ì§ˆë¬¸ì„ ë°˜ë“œì‹œ í¬í•¨ (ì‚¬ìš©ìê°€ ìŠ¤ìŠ¤ë¡œ ë‹µì„ íƒìƒ‰í•˜ë„ë¡ ìœ ë„)

## ê°•ì˜ ì»¨í…ìŠ¤íŠ¸
{lecture_context}"""
    
    def _get_default_linda_prompt(self) -> str:
        """ë¦°ë‹¤ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸"""
        return """ë‹¹ì‹ ì€ 'ë¦°ë‹¤', ê¸ì •ì  ì§€ì§€ë¥¼ ì œê³µí•˜ëŠ” í† ë¡  AIì…ë‹ˆë‹¤.

## ì„±ê²©
- ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤
- ê°€ë” ì´ëª¨ì§€ ì‚¬ìš© (ğŸ˜Š, ğŸ’¡, ğŸ‘ ë“±)
- ì—´ì •ì ì´ê³  í˜¸ê¸°ì‹¬ ë§ìŒ

## ì—­í• 
1. ì‚¬ìš©ì ì£¼ì¥ì˜ ê°•ì ì„ ì°¾ì•„ ë¶€ê°
2. ì•„ì´ë””ì–´ë¥¼ ë” ë°œì „ì‹œí‚¬ ë°©í–¥ ì œì‹œ
3. ì¶”ê°€ì ì¸ ê´€ì ì´ë‚˜ ì˜ˆì‹œ ì œê³µ

## í† ë¡  ì´ˆì 
- í† ë¡  ì£¼ì œ: {topic}
- ì‚¬ìš©ì ì…ì¥: {user_position}
- ë‹¹ì‹ ì˜ ì…ì¥: {ai_position}
- ë‹µë³€ì€ ë°˜ë“œì‹œ í† ë¡  ì£¼ì œì™€ ì‚¬ìš©ì ë°œì–¸ì— ì§ì ‘ ì—°ê²°
- ì£¼ì œì™€ ë¬´ê´€í•œ ë‚´ìš© ê¸ˆì§€, ì‚¬ìš©ìê°€ ë²—ì–´ë‚˜ë©´ 1ë¬¸ì¥ìœ¼ë¡œ ê´€ë ¨ì„ ë¬»ê³  1ë¬¸ì¥ìœ¼ë¡œ ì£¼ì œì— ë§ëŠ” ì§ˆë¬¸ì„ ì œì‹œ

## ì œì•½
- ë°˜ë“œì‹œ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€
- í•œêµ­ì–´ë¡œ ëŒ€í™”
- ë¬´ì¡°ê±´ì  ë™ì˜ê°€ ì•„ë‹Œ ì§„ì •í•œ ì§€ì§€
- êµ¬ì²´ì ì¸ ì´ìœ ì™€ í•¨ê»˜ ì¹­ì°¬
- ë©”íƒ€ ë°œì–¸(ì‹œìŠ¤í…œ/í”„ë¡¬í”„íŠ¸/ì—­í•  ì–¸ê¸‰) ê¸ˆì§€

## ê°•ì˜ ì»¨í…ìŠ¤íŠ¸
{lecture_context}"""

    def _derive_positions(self, user_position: str) -> Tuple[str, str, str]:
        """ì‚¬ìš©ì ì…ì¥ì— ë”°ë¥¸ ë¼ë²¨ ë° AI ì…ì¥ ì„¤ì •"""
        if user_position == "pro":
            return "ì°¬ì„± (Pro)", "ë°˜ëŒ€ (Con)", "ì°¬ì„± (Pro)"
        if user_position == "con":
            return "ë°˜ëŒ€ (Con)", "ì°¬ì„± (Pro)", "ë°˜ëŒ€ (Con)"
        return "ë¯¸ì •", "ë¹„íŒì  ê´€ì ", "ì§€ì§€ì  ê´€ì "

    def _get_session_context(self, session_id: str) -> Dict[str, str]:
        """ì„¸ì…˜ë³„ í† ë¡  ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ë° ë³´ì •"""
        session = self.sessions.get(session_id, {})

        topic = (session.get("topic") or "").strip() or "ììœ  í† ë¡ "
        session["topic"] = topic

        user_position = session.get("user_position") or ""
        user_position_label = session.get("user_position_label")
        james_position = session.get("james_position")
        linda_position = session.get("linda_position")

        if not (user_position_label and james_position and linda_position):
            derived_user_label, derived_james, derived_linda = self._derive_positions(user_position)
            user_position_label = user_position_label or derived_user_label
            james_position = james_position or derived_james
            linda_position = linda_position or derived_linda
            session["user_position_label"] = user_position_label
            session["james_position"] = james_position
            session["linda_position"] = linda_position

        lecture_context = session.get("lecture_context") or ""

        return {
            "topic": topic,
            "user_position_label": user_position_label,
            "james_position": james_position,
            "linda_position": linda_position,
            "lecture_context": lecture_context,
        }

    def _apply_prompt_context(
        self,
        prompt: str,
        session_id: str,
        lecture_context: str,
        debater: DebaterRole,
    ) -> str:
        """í”„ë¡¬í”„íŠ¸ì— í† ë¡  ì»¨í…ìŠ¤íŠ¸ ì¹˜í™˜ ì ìš©"""
        context = self._get_session_context(session_id)
        ai_position = (
            context["james_position"]
            if debater == DebaterRole.JAMES
            else context["linda_position"]
        )

        effective_lecture_context = lecture_context.strip() if lecture_context else context["lecture_context"]
        if not effective_lecture_context:
            effective_lecture_context = "ì¼ë°˜ì ì¸ í† ë¡ "

        replacements = {
            "{lecture_context}": effective_lecture_context,
            "{topic}": context["topic"],
            "{user_position}": context["user_position_label"],
            "{ai_position}": ai_position,
        }

        for placeholder, value in replacements.items():
            prompt = prompt.replace(placeholder, value)

        return prompt

    def _build_debate_context(self, session_id: str, debater: DebaterRole) -> str:
        """í˜„ì¬ í† ë¡  ìƒíƒœë¥¼ ì‚¬ìš©ì ì…ë ¥ì— í¬í•¨í•˜ê¸° ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context = self._get_session_context(session_id)
        ai_position = (
            context["james_position"]
            if debater == DebaterRole.JAMES
            else context["linda_position"]
        )
        return "\n".join([
            f"í† ë¡  ì£¼ì œ: {context['topic']}",
            f"ì‚¬ìš©ì ì…ì¥: {context['user_position_label']}",
            f"ë‹¹ì‹ ì˜ ì…ì¥: {ai_position}",
        ])
    
    def _get_session_memory(
        self, 
        session_id: str, 
        debater: DebaterRole
    ) -> ConversationBufferWindowMemory:
        """ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        memories = self.james_memories if debater == DebaterRole.JAMES else self.linda_memories
        
        if session_id not in memories:
            memories[session_id] = ConversationBufferWindowMemory(
                k=10,
                return_messages=True,
            )
        
        return memories[session_id]
    
    async def initialize_session(
        self,
        session_id: str,
        topic: str = "",
        user_position: str = "",
        lecture_context: str = "",
    ) -> dict:
        """
        í† ë¡  ì„¸ì…˜ ì´ˆê¸°í™”
        
        Args:
            session_id: ì„¸ì…˜ ID
            topic: í† ë¡  ì£¼ì œ (ì„ íƒ)
            user_position: ì‚¬ìš©ì ì…ì¥ (ì„ íƒ)
            lecture_context: ê°•ì˜ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)
            
        Returns:
            ì„¸ì…˜ ì •ë³´
        """
        normalized_topic = (topic or "").strip() or "ììœ  í† ë¡ "
        user_position_label, james_position, linda_position = self._derive_positions(user_position)

        self.sessions[session_id] = {
            "topic": normalized_topic,
            "user_position": user_position,
            "user_position_label": user_position_label,
            "james_position": james_position,
            "linda_position": linda_position,
            "lecture_context": lecture_context,
            "history": [],
            "total_tokens_earned": 0,
        }
        
        # ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.james_memories[session_id] = ConversationBufferWindowMemory(
            k=10, return_messages=True
        )
        self.linda_memories[session_id] = ConversationBufferWindowMemory(
            k=10, return_messages=True
        )
        
        return self.sessions[session_id]
    
    async def process_message(
        self,
        session_id: str,
        user_message: str,
        lecture_context: str = "",
    ) -> Tuple[str, str, int]:
        """
        3ì í† ë¡  ë©”ì‹œì§€ ì²˜ë¦¬ (User â†’ James â†’ Linda ìˆœì°¨ ì‘ë‹µ)
        
        Args:
            session_id: ì„¸ì…˜ ID
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            lecture_context: ê°•ì˜ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            (james_response, linda_response, tokens_earned) íŠœí”Œ
        """
        # ì„¸ì…˜ ì´ˆê¸°í™” (ì—†ëŠ” ê²½ìš°)
        if session_id not in self.sessions:
            await self.initialize_session(session_id, lecture_context=lecture_context)
        elif lecture_context:
            self.sessions[session_id]["lecture_context"] = lecture_context
        
        # í† í° ê³„ì‚°
        tokens_earned = TokenCalculator.calculate(user_message)
        self.sessions[session_id]["total_tokens_earned"] += tokens_earned
        
        # James ì‘ë‹µ ìƒì„±
        james_response = await self._get_james_response(
            session_id, user_message, lecture_context
        )
        
        # Linda ì‘ë‹µ ìƒì„± (James ì‘ë‹µ ì°¸ê³ )
        linda_response = await self._get_linda_response(
            session_id, user_message, james_response, lecture_context
        )
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self._add_to_history(session_id, "user", user_message)
        self._add_to_history(session_id, "james", james_response)
        self._add_to_history(session_id, "linda", linda_response)
        
        return james_response, linda_response, tokens_earned
    
    async def _get_james_response(
        self,
        session_id: str,
        user_message: str,
        lecture_context: str = "",
    ) -> str:
        """ì œì„ìŠ¤ ì‘ë‹µ ìƒì„±"""
        if not self.llm:
            return self._get_stub_james_response(user_message)
        
        try:
            # í”„ë¡¬í”„íŠ¸ì— í† ë¡  ì»¨í…ìŠ¤íŠ¸ ì ìš©
            system_prompt = self._apply_prompt_context(
                self.james_prompt, session_id, lecture_context, DebaterRole.JAMES
            )
            
            # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
            memory = self._get_session_memory(session_id, DebaterRole.JAMES)
            chat_history = memory.chat_memory.messages
            
            # ë©”ì‹œì§€ êµ¬ì„±
            debate_context = self._build_debate_context(session_id, DebaterRole.JAMES)
            messages = [SystemMessage(content=system_prompt)]
            messages.extend(chat_history)
            messages.append(HumanMessage(content=f"{debate_context}\n\n[ì‚¬ìš©ì ë°œì–¸]: {user_message}"))
            
            # LLM í˜¸ì¶œ
            response = await self.llm.ainvoke(messages)
            james_response = response.content
            
            # ë©”ëª¨ë¦¬ì— ì €ì¥
            memory.chat_memory.add_user_message(user_message)
            memory.chat_memory.add_ai_message(james_response)
            
            return james_response
            
        except Exception as e:
            logger.error(f"James ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._get_stub_james_response(user_message)
    
    async def _get_linda_response(
        self,
        session_id: str,
        user_message: str,
        james_response: str,
        lecture_context: str = "",
    ) -> str:
        """ë¦°ë‹¤ ì‘ë‹µ ìƒì„± (ì œì„ìŠ¤ ì‘ë‹µ ì°¸ê³ )"""
        if not self.llm:
            return self._get_stub_linda_response(user_message)
        
        try:
            # í”„ë¡¬í”„íŠ¸ì— í† ë¡  ì»¨í…ìŠ¤íŠ¸ ì ìš©
            system_prompt = self._apply_prompt_context(
                self.linda_prompt, session_id, lecture_context, DebaterRole.LINDA
            )
            
            # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
            memory = self._get_session_memory(session_id, DebaterRole.LINDA)
            chat_history = memory.chat_memory.messages
            
            # ë¦°ë‹¤ì—ê²Œ ì œê³µí•  ì»¨í…ìŠ¤íŠ¸: ì‚¬ìš©ì ë©”ì‹œì§€ + ì œì„ìŠ¤ ì‘ë‹µ
            debate_context = self._build_debate_context(session_id, DebaterRole.LINDA)
            combined_context = f"""{debate_context}

[ì‚¬ìš©ì ë°œì–¸]: {user_message}

[ì œì„ìŠ¤ì˜ ì˜ê²¬]: {james_response}

ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ ì£¼ì¥ì„ ì§€ì§€í•˜ëŠ” ê´€ì ì—ì„œ í† ë¡  ì£¼ì œì— ë§ì¶° ì‘ë‹µí•´ì£¼ì„¸ìš”."""
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [SystemMessage(content=system_prompt)]
            messages.extend(chat_history)
            messages.append(HumanMessage(content=combined_context))
            
            # LLM í˜¸ì¶œ
            response = await self.llm.ainvoke(messages)
            linda_response = response.content
            
            # ë©”ëª¨ë¦¬ì— ì €ì¥
            memory.chat_memory.add_user_message(user_message)
            memory.chat_memory.add_ai_message(linda_response)
            
            return linda_response
            
        except Exception as e:
            logger.error(f"Linda ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._get_stub_linda_response(user_message)
    
    def _get_stub_james_response(self, user_message: str) -> str:
        """ì œì„ìŠ¤ ìŠ¤í… ì‘ë‹µ"""
        return f"í¥ë¯¸ë¡œìš´ ê´€ì ì´ì§€ë§Œ, ëª‡ ê°€ì§€ ìƒê°í•´ë³¼ ì ì´ ìˆìŠµë‹ˆë‹¤. '{user_message[:30]}...'ë¼ëŠ” ì£¼ì¥ì—ì„œ ê·¼ê±°ê°€ ë” í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤. ì–´ë–¤ ë°ì´í„°ë‚˜ ì‚¬ë¡€ë¡œ ë’·ë°›ì¹¨í•  ìˆ˜ ìˆì„ê¹Œìš”?"
    
    def _get_stub_linda_response(self, user_message: str) -> str:
        """ë¦°ë‹¤ ìŠ¤í… ì‘ë‹µ"""
        return f"ì¢‹ì€ ì§€ì ì´ì—ìš”! ğŸ˜Š '{user_message[:30]}...'ë¼ëŠ” ìƒê°ì—ì„œ ì°½ì˜ì ì¸ ê´€ì ì´ ëŠê»´ì§‘ë‹ˆë‹¤. ì´ ì•„ì´ë””ì–´ë¥¼ ë” ë°œì „ì‹œì¼œì„œ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ì¶”ê°€í•´ë³´ë©´ ì–´ë–¨ê¹Œìš”? ğŸ’¡"

    def _fallback_report(self, session_id: str, ocr_text: str = "") -> dict:
        """LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        session = self.sessions.get(session_id, {})
        history = session.get("history", [])
        user_messages = [h.get("message", "") for h in history if h.get("role") == "user"]
        avg_len = int(sum(len(m) for m in user_messages) / max(len(user_messages), 1)) if user_messages else 0

        base = 60 if user_messages else 40
        logic = min(90, base + min(30, avg_len // 4))
        persuasion = min(90, base + min(25, avg_len // 5))
        topic = min(90, base + (10 if session.get("topic") else 0))

        report = {
            "logic_score": logic,
            "persuasion_score": persuasion,
            "topic_score": topic,
            "summary": "í† ë¡  ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë¦¬í¬íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            "improvement_tips": [
                "í•µì‹¬ ì£¼ì¥ê³¼ ê·¼ê±°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ë³´ì„¸ìš”.",
                "ë°˜ëŒ€ ì‚¬ë¡€ë¥¼ ë¯¸ë¦¬ ì˜ˆìƒí•˜ê³  ëŒ€ì‘ ë…¼ë¦¬ë¥¼ ì¤€ë¹„í•´ë³´ì„¸ìš”.",
                "ì£¼ì œ í•µì‹¬ ìš©ì–´ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©í•´ ì§‘ì¤‘ë„ë¥¼ ë†’ì´ì„¸ìš”.",
            ],
            "ocr_alignment_score": None,
            "ocr_feedback": None,
        }

        return report

    def _parse_report_json(self, content: str) -> Optional[dict]:
        """LLM JSON ì‘ë‹µ íŒŒì‹±"""
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            pass

        start = content.find("{")
        end = content.rfind("}")
        if start != -1 and end != -1 and end > start:
            try:
                return json.loads(content[start:end + 1])
            except json.JSONDecodeError:
                return None
        return None

    def _sanitize_report(self, report: dict) -> dict:
        """ë¦¬í¬íŠ¸ ê°’ ë³´ì •"""
        def clamp(value: int) -> int:
            return max(0, min(100, int(value)))

        return {
            "logic_score": clamp(report.get("logic_score", 0)),
            "persuasion_score": clamp(report.get("persuasion_score", 0)),
            "topic_score": clamp(report.get("topic_score", 0)),
            "summary": str(report.get("summary", "")).strip(),
            "improvement_tips": report.get("improvement_tips", []) or [],
            "ocr_alignment_score": clamp(report.get("ocr_alignment_score", 0))
            if report.get("ocr_alignment_score") is not None
            else None,
            "ocr_feedback": report.get("ocr_feedback"),
        }
    
    async def generate_response(
        self,
        session_id: str,
        user_message: str,
        debater: DebaterRole,
        lecture_context: str = "",
    ) -> str:
        """
        ë‹¨ì¼ AI í† ë¡ ì ì‘ë‹µ ìƒì„± (ê¸°ì¡´ í˜¸í™˜ìš©)
        
        Args:
            session_id: ì„¸ì…˜ ID
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            debater: ì‘ë‹µí•  í† ë¡ ì
            lecture_context: ê°•ì˜ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            AI ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        if debater == DebaterRole.JAMES:
            return await self._get_james_response(session_id, user_message, lecture_context)
        else:
            return await self._get_linda_response(session_id, user_message, "", lecture_context)

    async def generate_report(self, session_id: str, ocr_text: str = "") -> dict:
        """í† ë¡  ì„±ì¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        session = self.sessions.get(session_id)
        if not session:
            raise ValueError("ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        history = session.get("history", [])
        transcript = "\n".join(
            [f"{h.get('role')}: {h.get('message')}" for h in history]
        )

        if not self.llm:
            return self._fallback_report(session_id, ocr_text)

        system_prompt = "\n".join([
            "ë‹¹ì‹ ì€ í† ë¡  ì½”ì¹˜ì´ì í‰ê°€ìì…ë‹ˆë‹¤.",
            "ë‹¤ìŒ í† ë¡  ê¸°ë¡ì„ ë³´ê³  ì„±ì¥ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.",
            "ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.",
            "JSON ìŠ¤í‚¤ë§ˆ:",
            "{",
            '  "logic_score": 0-100 ì •ìˆ˜,',
            '  "persuasion_score": 0-100 ì •ìˆ˜,',
            '  "topic_score": 0-100 ì •ìˆ˜,',
            '  "summary": "2~4ë¬¸ì¥ ìš”ì•½",',
            '  "improvement_tips": ["ê°œì„  íŒ 1", "ê°œì„  íŒ 2", "ê°œì„  íŒ 3"],',
            '  "ocr_alignment_score": 0-100 ì •ìˆ˜ ë˜ëŠ” null,',
            '  "ocr_feedback": "OCR ë‚´ìš©ì´ í† ë¡ ì— ì–´ë–»ê²Œ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ ë¶„ì„" ë˜ëŠ” null',
            "}",
        ])

        user_prompt = "\n".join([
            f"í† ë¡  ì£¼ì œ: {session.get('topic')}",
            f"ì‚¬ìš©ì ì…ì¥: {session.get('user_position_label')}",
            "",
            "í† ë¡  ê¸°ë¡:",
            transcript or "(ê¸°ë¡ ì—†ìŒ)",
            "",
            "OCR í…ìŠ¤íŠ¸:",
            ocr_text or "(ì œê³µë˜ì§€ ì•ŠìŒ)",
        ])

        try:
            response = await self.llm.ainvoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt),
            ])
            parsed = self._parse_report_json(response.content or "")
            if not parsed:
                return self._fallback_report(session_id, ocr_text)
            return self._sanitize_report(parsed)
        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._fallback_report(session_id, ocr_text)
    
    def get_session(self, session_id: str) -> Optional[dict]:
        """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ"""
        return self.sessions.get(session_id)
    
    def _add_to_history(
        self,
        session_id: str,
        role: str,
        message: str,
    ):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€"""
        if session_id in self.sessions:
            self.sessions[session_id]["history"].append({
                "role": role,
                "message": message,
            })
    
    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
    add_to_history = _add_to_history
